{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PUC_protein_ligand.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "hBmY8Y05wFN6",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\"\"\"Bagging meta-estimator for PU learning.\"\"\"\n",
        "\n",
        "# Author: Gilles Louppe <g.louppe@gmail.com>\n",
        "# License: BSD 3 clause\n",
        "#\n",
        "#\n",
        "# Adapted for PU learning by Roy Wright <roy.w.wright@gmail.com>\n",
        "# (work in progress)\n",
        "#\n",
        "# A better idea: instead of a separate PU class, modify the original\n",
        "# sklearn BaggingClassifier so that the parameters `max_samples` \n",
        "# and `bootstrap` may be lists or dicts...\n",
        "# e.g. for a PU problem with 500 positives and 10000 unlabeled, we might set\n",
        "# max_samples = [500, 500]     (to balance P and U in each bag)\n",
        "# bootstrap = [True, False]    (to only bootstrap the unlabeled)\n",
        "\n",
        "\n",
        "from __future__ import division\n",
        "\n",
        "import itertools\n",
        "import numbers\n",
        "import numpy as np# type: ignore\n",
        "from warnings import warn\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "from sklearn.base import ClassifierMixin, RegressorMixin # type: ignore\n",
        "from sklearn.externals.joblib import Parallel, delayed# type: ignore\n",
        "from sklearn.externals.six import with_metaclass# type: ignore\n",
        "from sklearn.externals.six.moves import zip# type: ignore\n",
        "from sklearn.metrics import r2_score, accuracy_score# type: ignore\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor# type: ignore\n",
        "from sklearn.utils import check_random_state, check_X_y, check_array, column_or_1d# type: ignore\n",
        "from sklearn.utils.random import sample_without_replacement# type: ignore\n",
        "from sklearn.utils.validation import has_fit_parameter, check_is_fitted# type: ignore\n",
        "from sklearn.utils import indices_to_mask, check_consistent_length# type: ignore\n",
        "from sklearn.utils.metaestimators import if_delegate_has_method# type: ignore\n",
        "from sklearn.utils.multiclass import check_classification_targets# type: ignore\n",
        "\n",
        "from sklearn.ensemble.base import BaseEnsemble, _partition_estimators# type: ignore\n",
        "\n",
        "\n",
        "__all__ = [\"BaggingClassifierPU\"]\n",
        "\n",
        "MAX_INT = np.iinfo(np.int32).max\n",
        "\n",
        "\n",
        "def _generate_indices(random_state, bootstrap, n_population, n_samples):\n",
        "    \"\"\"Draw randomly sampled indices.\"\"\"\n",
        "    # Draw sample indices\n",
        "    if bootstrap:\n",
        "        indices = random_state.randint(0, n_population, n_samples)\n",
        "    else:\n",
        "        indices = sample_without_replacement(n_population, n_samples,\n",
        "                                             random_state=random_state)\n",
        "\n",
        "    return indices\n",
        "\n",
        "\n",
        "def _generate_bagging_indices(random_state, bootstrap_features,\n",
        "                              bootstrap_samples, n_features, n_samples,\n",
        "                              max_features, max_samples):\n",
        "    \"\"\"Randomly draw feature and sample indices.\"\"\"\n",
        "    # Get valid random state\n",
        "    random_state = check_random_state(random_state)\n",
        "\n",
        "    # Draw indices\n",
        "    feature_indices = _generate_indices(random_state, bootstrap_features,\n",
        "                                        n_features, max_features)\n",
        "    sample_indices = _generate_indices(random_state, bootstrap_samples,\n",
        "                                       n_samples, max_samples)\n",
        "\n",
        "    return feature_indices, sample_indices\n",
        "\n",
        "\n",
        "def _parallel_build_estimators(n_estimators, ensemble, X, y, sample_weight,\n",
        "                               seeds, total_n_estimators, verbose):\n",
        "    \"\"\"Private function used to build a batch of estimators within a job.\"\"\"\n",
        "    # Retrieve settings\n",
        "    n_samples, n_features = X.shape\n",
        "    max_features = ensemble._max_features\n",
        "    max_samples = ensemble._max_samples\n",
        "    bootstrap = ensemble.bootstrap\n",
        "    bootstrap_features = ensemble.bootstrap_features\n",
        "    support_sample_weight = has_fit_parameter(ensemble.base_estimator_,\n",
        "                                              \"sample_weight\")\n",
        "    if not support_sample_weight and sample_weight is not None:\n",
        "        raise ValueError(\"The base estimator doesn't support sample weight\")\n",
        "\n",
        "    # Build estimators\n",
        "    estimators = []\n",
        "    estimators_features = []\n",
        "\n",
        "    for i in range(n_estimators):\n",
        "        if verbose > 1:\n",
        "            print(\"Building estimator %d of %d for this parallel run \"\n",
        "                  \"(total %d)...\" % (i + 1, n_estimators, total_n_estimators))\n",
        "\n",
        "        random_state = np.random.RandomState(seeds[i])\n",
        "        estimator = ensemble._make_estimator(append=False,\n",
        "                                             random_state=random_state)\n",
        "\n",
        "        ################ MAIN MODIFICATION FOR PU LEARNING ##################\n",
        "        iP = [pair[0] for pair in enumerate(y) if pair[1] == 1]\n",
        "        iU = [pair[0] for pair in enumerate(y) if pair[1] < 1]            \n",
        "        features, indices = _generate_bagging_indices(random_state,\n",
        "                                                      bootstrap_features,\n",
        "                                                      bootstrap, n_features,\n",
        "                                                      len(iU), max_features,\n",
        "                                                      max_samples)\n",
        "        indices = [iU[i] for i in indices] + iP\n",
        "        #####################################################################\n",
        "        \n",
        "        \n",
        "        # Draw samples, using sample weights, and then fit\n",
        "        if support_sample_weight:\n",
        "            if sample_weight is None:\n",
        "                curr_sample_weight = np.ones((n_samples,))\n",
        "            else:\n",
        "                curr_sample_weight = sample_weight.copy()\n",
        "\n",
        "            if bootstrap:\n",
        "                sample_counts = np.bincount(indices, minlength=n_samples)\n",
        "                curr_sample_weight *= sample_counts\n",
        "            else:\n",
        "                not_indices_mask = ~indices_to_mask(indices, n_samples)\n",
        "                curr_sample_weight[not_indices_mask] = 0\n",
        "\n",
        "            estimator.fit(X[:, features], y, sample_weight=curr_sample_weight)\n",
        "\n",
        "        # Draw samples, using a mask, and then fit\n",
        "        else:\n",
        "            estimator.fit((X[indices])[:, features], y[indices])\n",
        "\n",
        "        estimators.append(estimator)\n",
        "        estimators_features.append(features)\n",
        "\n",
        "    return estimators, estimators_features\n",
        "\n",
        "\n",
        "def _parallel_predict_proba(estimators, estimators_features, X, n_classes):\n",
        "    \"\"\"Private function used to compute (proba-)predictions within a job.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    proba = np.zeros((n_samples, n_classes))\n",
        "\n",
        "    for estimator, features in zip(estimators, estimators_features):\n",
        "        if hasattr(estimator, \"predict_proba\"):\n",
        "            proba_estimator = estimator.predict_proba(X[:, features])\n",
        "\n",
        "            if n_classes == len(estimator.classes_):\n",
        "                proba += proba_estimator\n",
        "\n",
        "            else:\n",
        "                proba[:, estimator.classes_] += \\\n",
        "                    proba_estimator[:, range(len(estimator.classes_))]\n",
        "\n",
        "        else:\n",
        "            # Resort to voting\n",
        "            predictions = estimator.predict(X[:, features])\n",
        "\n",
        "            for i in range(n_samples):\n",
        "                proba[i, predictions[i]] += 1\n",
        "\n",
        "    return proba\n",
        "\n",
        "\n",
        "def _parallel_predict_log_proba(estimators, estimators_features, X, n_classes):\n",
        "    \"\"\"Private function used to compute log probabilities within a job.\"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "    log_proba = np.empty((n_samples, n_classes))\n",
        "    log_proba.fill(-np.inf)\n",
        "    all_classes = np.arange(n_classes, dtype=np.int)\n",
        "\n",
        "    for estimator, features in zip(estimators, estimators_features):\n",
        "        log_proba_estimator = estimator.predict_log_proba(X[:, features])\n",
        "\n",
        "        if n_classes == len(estimator.classes_):\n",
        "            log_proba = np.logaddexp(log_proba, log_proba_estimator)\n",
        "\n",
        "        else:\n",
        "            log_proba[:, estimator.classes_] = np.logaddexp(\n",
        "                log_proba[:, estimator.classes_],\n",
        "                log_proba_estimator[:, range(len(estimator.classes_))])\n",
        "\n",
        "            missing = np.setdiff1d(all_classes, estimator.classes_)\n",
        "            log_proba[:, missing] = np.logaddexp(log_proba[:, missing],\n",
        "                                                 -np.inf)\n",
        "\n",
        "    return log_proba\n",
        "\n",
        "\n",
        "def _parallel_decision_function(estimators, estimators_features, X):\n",
        "    \"\"\"Private function used to compute decisions within a job.\"\"\"\n",
        "    return sum(estimator.decision_function(X[:, features])\n",
        "               for estimator, features in zip(estimators,\n",
        "                                              estimators_features))\n",
        "\n",
        "\n",
        "class BaseBaggingPU(with_metaclass(ABCMeta, BaseEnsemble)):\n",
        "    \"\"\"Base class for Bagging PU meta-estimator.\n",
        "    Warning: This class should not be used directly. Use derived classes\n",
        "    instead.\n",
        "    \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __init__(self,\n",
        "                 base_estimator=None,\n",
        "                 n_estimators=10,\n",
        "                 max_samples=1.0,\n",
        "                 max_features=1.0,\n",
        "                 bootstrap=True,\n",
        "                 bootstrap_features=False,\n",
        "                 oob_score=True,\n",
        "                 warm_start=False,\n",
        "                 n_jobs=1,\n",
        "                 random_state=None,\n",
        "                 verbose=0):\n",
        "        super(BaseBaggingPU, self).__init__(\n",
        "            base_estimator=base_estimator,\n",
        "            n_estimators=n_estimators)\n",
        "\n",
        "        self.max_samples = max_samples\n",
        "        self.max_features = max_features\n",
        "        self.bootstrap = bootstrap\n",
        "        self.bootstrap_features = bootstrap_features\n",
        "        self.oob_score = oob_score\n",
        "        self.warm_start = warm_start\n",
        "        self.n_jobs = n_jobs\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        \"\"\"Build a Bagging ensemble of estimators from the training\n",
        "           set (X, y).\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
        "            The training input samples. Sparse matrices are accepted only if\n",
        "            they are supported by the base estimator.\n",
        "        y : array-like, shape = [n_samples]\n",
        "            The target values (1 for positive, 0 for unlabeled).\n",
        "        sample_weight : array-like, shape = [n_samples] or None\n",
        "            Sample weights. If None, then samples are equally weighted.\n",
        "            Note that this is supported only if the base estimator supports\n",
        "            sample weighting.\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        return self._fit(X, y, self.max_samples, sample_weight=sample_weight)\n",
        "\n",
        "    def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):\n",
        "        \"\"\"Build a Bagging ensemble of estimators from the training\n",
        "           set (X, y).\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
        "            The training input samples. Sparse matrices are accepted only if\n",
        "            they are supported by the base estimator.\n",
        "        y : array-like, shape = [n_samples]\n",
        "            The target values (1 for positive, 0 for unlabeled).\n",
        "        max_samples : int or float, optional (default=None)\n",
        "            Argument to use instead of self.max_samples.\n",
        "        max_depth : int, optional (default=None)\n",
        "            Override value used when constructing base estimator. Only\n",
        "            supported if the base estimator has a max_depth parameter.\n",
        "        sample_weight : array-like, shape = [n_samples] or None\n",
        "            Sample weights. If None, then samples are equally weighted.\n",
        "            Note that this is supported only if the base estimator supports\n",
        "            sample weighting.\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        self.y = y\n",
        "        \n",
        "        # Convert data\n",
        "        X, y = check_X_y(X, y, ['csr', 'csc'])\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = check_array(sample_weight, ensure_2d=False)\n",
        "            check_consistent_length(y, sample_weight)\n",
        "\n",
        "        # Remap output\n",
        "        n_samples, self.n_features_ = X.shape\n",
        "        self._n_samples = n_samples\n",
        "        y = self._validate_y(y)\n",
        "\n",
        "        # Check parameters\n",
        "        self._validate_estimator()\n",
        "\n",
        "        if max_depth is not None:\n",
        "            self.base_estimator_.max_depth = max_depth\n",
        "\n",
        "        # Validate max_samples\n",
        "        if max_samples is None:\n",
        "            max_samples = self.max_samples\n",
        "        elif not isinstance(max_samples, (numbers.Integral, np.integer)):\n",
        "            max_samples = int(max_samples * sum(y < 1))\n",
        "\n",
        "        if not (0 < max_samples <= sum(y < 1)):\n",
        "            raise ValueError(\"max_samples must be positive\"\n",
        "                             \" and no larger than the number of unlabeled points\")\n",
        "\n",
        "        # Store validated integer row sampling value\n",
        "        self._max_samples = max_samples\n",
        "\n",
        "        # Validate max_features\n",
        "        if isinstance(self.max_features, (numbers.Integral, np.integer)):\n",
        "            max_features = self.max_features\n",
        "        else:  # float\n",
        "            max_features = int(self.max_features * self.n_features_)\n",
        "\n",
        "        if not (0 < max_features <= self.n_features_):\n",
        "            raise ValueError(\"max_features must be in (0, n_features]\")\n",
        "\n",
        "        # Store validated integer feature sampling value\n",
        "        self._max_features = max_features\n",
        "\n",
        "        # Other checks\n",
        "        if not self.bootstrap and self.oob_score:\n",
        "            raise ValueError(\"Out of bag estimation only available\"\n",
        "                             \" if bootstrap=True\")\n",
        "\n",
        "        if self.warm_start and self.oob_score:\n",
        "            raise ValueError(\"Out of bag estimate only available\"\n",
        "                             \" if warm_start=False\")\n",
        "\n",
        "        if hasattr(self, \"oob_score_\") and self.warm_start:\n",
        "            del self.oob_score_\n",
        "\n",
        "        if not self.warm_start or not hasattr(self, 'estimators_'):\n",
        "            # Free allocated memory, if any\n",
        "            self.estimators_ = []\n",
        "            self.estimators_features_ = []\n",
        "\n",
        "        n_more_estimators = self.n_estimators - len(self.estimators_)\n",
        "\n",
        "        if n_more_estimators < 0:\n",
        "            raise ValueError('n_estimators=%d must be larger or equal to '\n",
        "                             'len(estimators_)=%d when warm_start==True'\n",
        "                             % (self.n_estimators, len(self.estimators_)))\n",
        "\n",
        "        elif n_more_estimators == 0:\n",
        "            warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
        "                 \"fit new trees.\")\n",
        "            return self\n",
        "\n",
        "        # Parallel loop\n",
        "        n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,\n",
        "                                                             self.n_jobs)\n",
        "        total_n_estimators = sum(n_estimators)\n",
        "\n",
        "        # Advance random state to state after training\n",
        "        # the first n_estimators\n",
        "        if self.warm_start and len(self.estimators_) > 0:\n",
        "            random_state.randint(MAX_INT, size=len(self.estimators_))\n",
        "\n",
        "        seeds = random_state.randint(MAX_INT, size=n_more_estimators)\n",
        "        self._seeds = seeds\n",
        "\n",
        "        all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
        "            delayed(_parallel_build_estimators)(\n",
        "                n_estimators[i],\n",
        "                self,\n",
        "                X,\n",
        "                y,\n",
        "                sample_weight,\n",
        "                seeds[starts[i]:starts[i + 1]],\n",
        "                total_n_estimators,\n",
        "                verbose=self.verbose)\n",
        "            for i in range(n_jobs))\n",
        "\n",
        "        # Reduce\n",
        "        self.estimators_ += list(itertools.chain.from_iterable(\n",
        "            t[0] for t in all_results))\n",
        "        self.estimators_features_ += list(itertools.chain.from_iterable(\n",
        "            t[1] for t in all_results))\n",
        "\n",
        "        if self.oob_score:\n",
        "            self._set_oob_score(X, y)\n",
        "\n",
        "        return self\n",
        "\n",
        "    @abstractmethod\n",
        "    def _set_oob_score(self, X, y):\n",
        "        \"\"\"Calculate out of bag predictions and score.\"\"\"\n",
        "\n",
        "    def _validate_y(self, y):\n",
        "        # Default implementation\n",
        "        return column_or_1d(y, warn=True)\n",
        "\n",
        "    def _get_estimators_indices(self):\n",
        "        # Get drawn indices along both sample and feature axes\n",
        "        for seed in self._seeds:\n",
        "            # Operations accessing random_state must be performed identically\n",
        "            # to those in `_parallel_build_estimators()`\n",
        "            random_state = np.random.RandomState(seed)\n",
        "            \n",
        "            ############ MAIN MODIFICATION FOR PU LEARNING ###############\n",
        "            iP = [pair[0] for pair in enumerate(self.y) if pair[1] == 1]\n",
        "            iU = [pair[0] for pair in enumerate(self.y) if pair[1] < 1] \n",
        "            \n",
        "            feature_indices, sample_indices = _generate_bagging_indices(\n",
        "                random_state, self.bootstrap_features, self.bootstrap,\n",
        "                self.n_features_, len(iU), self._max_features,\n",
        "                self._max_samples)\n",
        "\n",
        "            sample_indices = [iU[i] for i in sample_indices] + iP\n",
        "            ###############################################################\n",
        "            \n",
        "            yield feature_indices, sample_indices\n",
        "\n",
        "    @property\n",
        "    def estimators_samples_(self):\n",
        "        \"\"\"The subset of drawn samples for each base estimator.\n",
        "        Returns a dynamically generated list of boolean masks identifying\n",
        "        the samples used for fitting each member of the ensemble, i.e.,\n",
        "        the in-bag samples.\n",
        "        Note: the list is re-created at each call to the property in order\n",
        "        to reduce the object memory footprint by not storing the sampling\n",
        "        data. Thus fetching the property may be slower than expected.\n",
        "        \"\"\"\n",
        "        sample_masks = []\n",
        "        for _, sample_indices in self._get_estimators_indices():\n",
        "            mask = indices_to_mask(sample_indices, self._n_samples)\n",
        "            sample_masks.append(mask)\n",
        "\n",
        "        return sample_masks\n",
        "\n",
        "\n",
        "class BaggingClassifierPU(BaseBaggingPU, ClassifierMixin):\n",
        "    \"\"\"A Bagging PU classifier.\n",
        "    Adapted from sklearn.ensemble.BaggingClassifier, based on\n",
        "    A bagging SVM to learn from positive and unlabeled examples (2013) by Mordelet and Vert\n",
        "    http://dx.doi.org/10.1016/j.patrec.2013.06.010\n",
        "    http://members.cbio.mines-paristech.fr/~jvert/svn/bibli/local/Mordelet2013bagging.pdf\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    base_estimator : object or None, optional (default=None)\n",
        "        The base estimator to fit on random subsets of the dataset.\n",
        "        If None, then the base estimator is a decision tree.\n",
        "    n_estimators : int, optional (default=10)\n",
        "        The number of base estimators in the ensemble.\n",
        "    max_samples : int or float, optional (default=1.0)\n",
        "        The number of unlabeled samples to draw to train each base estimator.\n",
        "    max_features : int or float, optional (default=1.0)\n",
        "        The number of features to draw from X to train each base estimator.\n",
        "        - If int, then draw `max_features` features.\n",
        "        - If float, then draw `max_features * X.shape[1]` features.\n",
        "    bootstrap : boolean, optional (default=True)\n",
        "        Whether samples are drawn with replacement.\n",
        "    bootstrap_features : boolean, optional (default=False)\n",
        "        Whether features are drawn with replacement.\n",
        "    oob_score : bool, optional (default=True)\n",
        "        Whether to use out-of-bag samples to estimate\n",
        "        the generalization error.\n",
        "    warm_start : bool, optional (default=False)\n",
        "        When set to True, reuse the solution of the previous call to fit\n",
        "        and add more estimators to the ensemble, otherwise, just fit\n",
        "        a whole new ensemble.\n",
        "    n_jobs : int, optional (default=1)\n",
        "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
        "        If -1, then the number of jobs is set to the number of cores.\n",
        "    random_state : int, RandomState instance or None, optional (default=None)\n",
        "        If int, random_state is the seed used by the random number generator;\n",
        "        If RandomState instance, random_state is the random number generator;\n",
        "        If None, the random number generator is the RandomState instance used\n",
        "        by `np.random`.\n",
        "    verbose : int, optional (default=0)\n",
        "        Controls the verbosity of the building process.\n",
        "    Attributes\n",
        "    ----------\n",
        "    base_estimator_ : estimator\n",
        "        The base estimator from which the ensemble is grown.\n",
        "    estimators_ : list of estimators\n",
        "        The collection of fitted base estimators.\n",
        "    estimators_samples_ : list of arrays\n",
        "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
        "        estimator. Each subset is defined by a boolean mask.\n",
        "    estimators_features_ : list of arrays\n",
        "        The subset of drawn features for each base estimator.\n",
        "    classes_ : array of shape = [n_classes]\n",
        "        The classes labels.\n",
        "    n_classes_ : int or list\n",
        "        The number of classes.\n",
        "    oob_score_ : float\n",
        "        Score of the training dataset obtained using an out-of-bag estimate.\n",
        "    oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
        "        Decision function computed with out-of-bag estimate on the training\n",
        "        set. Positive data points, and perhaps some of the unlabeled,\n",
        "        are left out during the bootstrap. In these cases,\n",
        "        `oob_decision_function_` contains NaN.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 base_estimator=None,\n",
        "                 n_estimators=10,\n",
        "                 max_samples=1.0,\n",
        "                 max_features=1.0,\n",
        "                 bootstrap=True,\n",
        "                 bootstrap_features=False,\n",
        "                 oob_score=True,\n",
        "                 warm_start=False,\n",
        "                 n_jobs=1,\n",
        "                 random_state=None,\n",
        "                 verbose=0):\n",
        "\n",
        "        super(BaggingClassifierPU, self).__init__(\n",
        "            base_estimator,\n",
        "            n_estimators=n_estimators,\n",
        "            max_samples=max_samples,\n",
        "            max_features=max_features,\n",
        "            bootstrap=bootstrap,\n",
        "            bootstrap_features=bootstrap_features,\n",
        "            oob_score=oob_score,\n",
        "            warm_start=warm_start,\n",
        "            n_jobs=n_jobs,\n",
        "            random_state=random_state,\n",
        "            verbose=verbose)\n",
        "\n",
        "    def _validate_estimator(self):\n",
        "        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n",
        "        super(BaggingClassifierPU, self)._validate_estimator(\n",
        "            default=DecisionTreeClassifier())\n",
        "\n",
        "    def _set_oob_score(self, X, y):\n",
        "        n_samples = y.shape[0]\n",
        "        n_classes_ = self.n_classes_\n",
        "        classes_ = self.classes_\n",
        "\n",
        "        predictions = np.zeros((n_samples, n_classes_))\n",
        "\n",
        "        for estimator, samples, features in zip(self.estimators_,\n",
        "                                                self.estimators_samples_,\n",
        "                                                self.estimators_features_):\n",
        "            # Create mask for OOB samples\n",
        "            mask = ~samples\n",
        "\n",
        "            if hasattr(estimator, \"predict_proba\"):\n",
        "                predictions[mask, :] += estimator.predict_proba(\n",
        "                    (X[mask, :])[:, features])\n",
        "\n",
        "            else:\n",
        "                p = estimator.predict((X[mask, :])[:, features])\n",
        "                j = 0\n",
        "\n",
        "                for i in range(n_samples):\n",
        "                    if mask[i]:\n",
        "                        predictions[i, p[j]] += 1\n",
        "                        j += 1\n",
        "\n",
        "                        \n",
        "        # Modified: no warnings about non-OOB points (i.e. positives)\n",
        "        with np.errstate(invalid='ignore'):  \n",
        "            oob_decision_function = (predictions /\n",
        "                                     predictions.sum(axis=1)[:, np.newaxis])\n",
        "            oob_score = accuracy_score(y, np.argmax(predictions, axis=1))\n",
        "\n",
        "        self.oob_decision_function_ = oob_decision_function\n",
        "        self.oob_score_ = oob_score\n",
        "\n",
        "    def _validate_y(self, y):\n",
        "        y = column_or_1d(y, warn=True)\n",
        "        check_classification_targets(y)\n",
        "        self.classes_, y = np.unique(y, return_inverse=True)\n",
        "        self.n_classes_ = len(self.classes_)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class for X.\n",
        "        The predicted class of an input sample is computed as the class with\n",
        "        the highest mean predicted probability. If base estimators do not\n",
        "        implement a ``predict_proba`` method, then it resorts to voting.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
        "            The training input samples. Sparse matrices are accepted only if\n",
        "            they are supported by the base estimator.\n",
        "        Returns\n",
        "        -------\n",
        "        y : array of shape = [n_samples]\n",
        "            The predicted classes.\n",
        "        \"\"\"\n",
        "        predicted_probabilitiy = self.predict_proba(X)\n",
        "        return self.classes_.take((np.argmax(predicted_probabilitiy, axis=1)),\n",
        "                                  axis=0)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict class probabilities for X.\n",
        "        The predicted class probabilities of an input sample is computed as\n",
        "        the mean predicted class probabilities of the base estimators in the\n",
        "        ensemble. If base estimators do not implement a ``predict_proba``\n",
        "        method, then it resorts to voting and the predicted class probabilities\n",
        "        of an input sample represents the proportion of estimators predicting\n",
        "        each class.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
        "            The training input samples. Sparse matrices are accepted only if\n",
        "            they are supported by the base estimator.\n",
        "        Returns\n",
        "        -------\n",
        "        p : array of shape = [n_samples, n_classes]\n",
        "            The class probabilities of the input samples. The order of the\n",
        "            classes corresponds to that in the attribute `classes_`.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, \"classes_\")\n",
        "        # Check data\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc'])\n",
        "\n",
        "        if self.n_features_ != X.shape[1]:\n",
        "            raise ValueError(\"Number of features of the model must \"\n",
        "                             \"match the input. Model n_features is {0} and \"\n",
        "                             \"input n_features is {1}.\"\n",
        "                             \"\".format(self.n_features_, X.shape[1]))\n",
        "\n",
        "        # Parallel loop\n",
        "        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,\n",
        "                                                             self.n_jobs)\n",
        "\n",
        "        all_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
        "            delayed(_parallel_predict_proba)(\n",
        "                self.estimators_[starts[i]:starts[i + 1]],\n",
        "                self.estimators_features_[starts[i]:starts[i + 1]],\n",
        "                X,\n",
        "                self.n_classes_)\n",
        "            for i in range(n_jobs))\n",
        "\n",
        "        # Reduce\n",
        "        proba = sum(all_proba) / self.n_estimators\n",
        "\n",
        "        return proba\n",
        "\n",
        "    def predict_log_proba(self, X):\n",
        "        \"\"\"Predict class log-probabilities for X.\n",
        "        The predicted class log-probabilities of an input sample is computed as\n",
        "        the log of the mean predicted class probabilities of the base\n",
        "        estimators in the ensemble.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
        "            The training input samples. Sparse matrices are accepted only if\n",
        "            they are supported by the base estimator.\n",
        "        Returns\n",
        "        -------\n",
        "        p : array of shape = [n_samples, n_classes]\n",
        "            The class log-probabilities of the input samples. The order of the\n",
        "            classes corresponds to that in the attribute `classes_`.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, \"classes_\")\n",
        "        if hasattr(self.base_estimator_, \"predict_log_proba\"):\n",
        "            # Check data\n",
        "            X = check_array(X, accept_sparse=['csr', 'csc'])\n",
        "\n",
        "            if self.n_features_ != X.shape[1]:\n",
        "                raise ValueError(\"Number of features of the model must \"\n",
        "                                 \"match the input. Model n_features is {0} \"\n",
        "                                 \"and input n_features is {1} \"\n",
        "                                 \"\".format(self.n_features_, X.shape[1]))\n",
        "\n",
        "            # Parallel loop\n",
        "            n_jobs, n_estimators, starts = _partition_estimators(\n",
        "                self.n_estimators, self.n_jobs)\n",
        "\n",
        "            all_log_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
        "                delayed(_parallel_predict_log_proba)(\n",
        "                    self.estimators_[starts[i]:starts[i + 1]],\n",
        "                    self.estimators_features_[starts[i]:starts[i + 1]],\n",
        "                    X,\n",
        "                    self.n_classes_)\n",
        "                for i in range(n_jobs))\n",
        "\n",
        "            # Reduce\n",
        "            log_proba = all_log_proba[0]\n",
        "\n",
        "            for j in range(1, len(all_log_proba)):\n",
        "                log_proba = np.logaddexp(log_proba, all_log_proba[j])\n",
        "\n",
        "            log_proba -= np.log(self.n_estimators)\n",
        "\n",
        "            return log_proba\n",
        "\n",
        "        else:\n",
        "            return np.log(self.predict_proba(X))\n",
        "\n",
        "    @if_delegate_has_method(delegate='base_estimator')\n",
        "    def decision_function(self, X):\n",
        "        \"\"\"Average of the decision functions of the base classifiers.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
        "            The training input samples. Sparse matrices are accepted only if\n",
        "            they are supported by the base estimator.\n",
        "        Returns\n",
        "        -------\n",
        "        score : array, shape = [n_samples, k]\n",
        "            The decision function of the input samples. The columns correspond\n",
        "            to the classes in sorted order, as they appear in the attribute\n",
        "            ``classes_``. Regression and binary classification are special\n",
        "            cases with ``k == 1``, otherwise ``k==n_classes``.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, \"classes_\")\n",
        "\n",
        "        # Check data\n",
        "        X = check_array(X, accept_sparse=['csr', 'csc'])\n",
        "\n",
        "        if self.n_features_ != X.shape[1]:\n",
        "            raise ValueError(\"Number of features of the model must \"\n",
        "                             \"match the input. Model n_features is {0} and \"\n",
        "                             \"input n_features is {1} \"\n",
        "                             \"\".format(self.n_features_, X.shape[1]))\n",
        "\n",
        "        # Parallel loop\n",
        "        n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators,\n",
        "                                                             self.n_jobs)\n",
        "\n",
        "        all_decisions = Parallel(n_jobs=n_jobs, verbose=self.verbose)(\n",
        "            delayed(_parallel_decision_function)(\n",
        "                self.estimators_[starts[i]:starts[i + 1]],\n",
        "                self.estimators_features_[starts[i]:starts[i + 1]],\n",
        "                X)\n",
        "            for i in range(n_jobs))\n",
        "\n",
        "        # Reduce\n",
        "        decisions = sum(all_decisions) / self.n_estimators\n",
        "\n",
        "        return decisions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5rNR6P4OuOMd",
        "colab_type": "code",
        "outputId": "f1e3b9ed-2e16-42fb-a373-5634d749c22f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "'''make vectorizer from sequences csv'''\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore\n",
        "from sklearn.tree import DecisionTreeClassifier # type: ignore\n",
        "import pandas as pd  # type: ignore\n",
        "import pickle\n",
        "import numpy as np  # type: ignore\n",
        "import argparse\n",
        "from subprocess import call\n",
        "\n",
        "raw_prefix = 'https://raw.githubusercontent.com/prescriptive-possibilities-april-15-19/mocking/master/'\n",
        "\n",
        "def seq_vectorizer(\n",
        "        min_f_count: int = 10,\n",
        "        ngram_max: int = 10,\n",
        "        max_features: int = 10000,\n",
        "        downsample: int = -1\n",
        "        ) -> TfidfVectorizer:\n",
        "    if downsample < 16:\n",
        "        sequences = pd.read_csv(\n",
        "            raw_prefix + 'sequences.csv').rename({'Unnamed: 0': 'seq_id'}, axis=1)\n",
        "    else:\n",
        "        sequences = pd.read_csv(raw_prefix + 'sequences.csv').rename(\n",
        "            {'Unnamed: 0': 'seq_id'}, axis=1).sample(downsample)\n",
        "\n",
        "    tfidf = TfidfVectorizer(\n",
        "        lowercase=False,\n",
        "        analyzer='char',\n",
        "        stop_words=None,\n",
        "        ngram_range=(1, ngram_max),\n",
        "        min_df=min_f_count,\n",
        "        max_features=max_features\n",
        "    )\n",
        "\n",
        "    print(\"training data: \", sequences.shape)\n",
        "\n",
        "    print(\"fitting........\") \n",
        "    tfidf.fit(sequences.sequence.values)\n",
        "    print(\"all trained up!\")\n",
        "    return tfidf\n",
        "\n",
        "tfidf = seq_vectorizer(ngram_max=4, downsample=30000)\n",
        "\n",
        "tfidf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data:  (30000, 2)\n",
            "fitting........\n",
            "all trained up!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
              "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
              "        lowercase=False, max_df=1.0, max_features=10000, min_df=10,\n",
              "        ngram_range=(1, 4), norm='l2', preprocessor=None, smooth_idf=True,\n",
              "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
              "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
              "        vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "Tdqh0X8NuLCZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "lig_num = 1000\n",
        "sample_size = 200\n",
        "estimators = 100\n",
        "\n",
        "downsample_binding=230000\n",
        "downsample_sequences=20000\n",
        "\n",
        "def init_dat(lig_num: int = lig_num, sample_size: int = sample_size,\n",
        "             estimators: int = estimators, downsample_binding: int = -1, \n",
        "             downsample_sequences: int = -1):\n",
        "\n",
        "    if downsample_binding < 16:\n",
        "        lig2seq = pd.read_csv(\n",
        "            raw_prefix + 'lig2seq.csv').rename({'lig': 'lig_id', \n",
        "                                                'seq': 'seq_id'}, axis=1)\n",
        "    else:\n",
        "        lig2seq = pd.read_csv(\n",
        "            raw_prefix +\n",
        "            'lig2seq.csv').rename(\n",
        "            {\n",
        "                'lig': 'lig_id',\n",
        "                'seq': 'seq_id'},\n",
        "            axis=1).sample(downsample_binding)\n",
        "\n",
        "    if downsample_sequences < 16:\n",
        "        sequences = pd.read_csv(\n",
        "            raw_prefix + 'sequences.csv').rename({'Unnamed: 0': \n",
        "                                                  'seq_id'}, axis=1)\n",
        "    else:\n",
        "        sequences = pd.read_csv(raw_prefix + 'sequences.csv').rename(\n",
        "            {'Unnamed: 0': 'seq_id'}, axis=1).sample(downsample_sequences)\n",
        "\n",
        "    lig_id_vals = list(np.random.choice(lig2seq.lig_id.unique(), size=lig_num))\n",
        "    binding = lig2seq.loc[lig2seq.lig_id.isin(lig_id_vals)]\n",
        "\n",
        "    return sequences, lig_id_vals, binding\n",
        "\n",
        "\n",
        "sequences, lig_id_vals, binding = init_dat(\n",
        "    downsample_binding=downsample_binding, \n",
        "    downsample_sequences=downsample_sequences\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j4B-eEbMxxAs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "eu3nwxxsuLDH",
        "colab_type": "code",
        "outputId": "16c38355-c239-4291-acfd-ef9976e6c2bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1245
        }
      },
      "cell_type": "code",
      "source": [
        "#from utils.baggingPU import BaggingClassifierPU\n",
        "# BaggingClassifierPU is in current namespace because entire baggingPU.py code is in a cell above. \n",
        "\n",
        "models = {}\n",
        "for i, lig_id in enumerate(lig_id_vals):\n",
        "    print(i, len(lig_id_vals), len(models))\n",
        "    seq_id = list(binding.loc[binding.lig_id==lig_id, \"seq\"])\n",
        "    if len(seq_id) < sample_size//2 and len(seq_id) > 5:\n",
        "        df_seq_sub_neg = sequences.loc[~sequences.index.isin(seq_id)].sample(n=sample_size-len(seq_id), random_state=seed+i)\n",
        "        df_seq_sub_neg.loc[:,\"bind\"] = 0\n",
        "        df_seq_sub_neg.loc[:,\"bind_t\"] = 0\n",
        "        df_seq_sub_pos = sequences.loc[sequences.index.isin(seq_id)].sample(frac=0.75, random_state=seed+i)\n",
        "        df_seq_sub_pos.loc[:,\"bind\"] = 1\n",
        "        df_seq_sub_pos.loc[:,\"bind_t\"] = 1\n",
        "        df_seq_sub_pos_hid_0 = sequences.loc[sequences.index.isin(seq_id)]\n",
        "        df_seq_sub_pos_hid = df_seq_sub_pos_hid_0.loc[~df_seq_sub_pos_hid_0.index.isin(df_seq_sub_pos.index)]\n",
        "        df_seq_sub_pos_hid.loc[:,\"bind\"] = 0\n",
        "        df_seq_sub_pos_hid.loc[:,\"bind_t\"] = 1\n",
        "        \n",
        "        df_fitter = pd.concat([df_seq_sub_neg, df_seq_sub_pos, df_seq_sub_pos_hid])\n",
        "        df_fitter[\"sequence\"] = df_fitter[\"sequence\"].apply(lambda x: tfidf.transform([x]))\n",
        "        \n",
        "        \n",
        "        X = np.array([df_fitter[\"sequence\"].values[i].toarray()[0] for i in range(df_fitter[\"sequence\"].values.shape[0])])\n",
        "        y = df_fitter[\"bind\"].values\n",
        "        \n",
        "        bc = BaggingClassifierPU(DecisionTreeClassifier(), n_estimators=estimators, n_jobs=-1, max_samples=sum(y))\n",
        "        bc.fit(X, y)\n",
        "        df_seq_pos_all = sequences.loc[sequences.index.isin(seq_id)]\n",
        "        df_seq_pos_all[\"seq_vec\"] = df_seq_pos_all[\"sequence\"].apply(lambda x: tfidf.transform([x]))\n",
        "        df_seq_sub_neg[\"seq_vec\"] = df_seq_sub_neg[\"sequence\"].apply(lambda x: tfidf.transform([x]))\n",
        "        print(\n",
        "            lig_id,\n",
        "            len(seq_id),\n",
        "            bc.predict(np.array([df_seq_pos_all[\"seq_vec\"].values[i].toarray()[0] for i in range(df_seq_pos_all[\"seq_vec\"].values.shape[0])])).sum()/df_seq_pos_all.shape[0],\n",
        "            bc.predict(np.array([df_seq_sub_neg[\"seq_vec\"].values[i].toarray()[0] for i in range(df_seq_sub_neg[\"seq_vec\"].values.shape[0])])).sum()/df_seq_sub_neg.shape[0]\n",
        "        )\n",
        "        models[lig_id] = pickle.dumps(bc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1000 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1789\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m                     \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36merror\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1784\u001b[0m                                .format(key=key,\n\u001b[0;32m-> 1785\u001b[0;31m                                        axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'the label [seq] is not in the [columns]'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c079edd4c7ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlig_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlig_id_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlig_id_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mseq_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbinding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlig_id\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlig_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seq\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdf_seq_sub_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1470\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_label_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m                 \u001b[0;31m# we have yielded a scalar ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1796\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1798\u001b[0;31m                 \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36merror\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 raise KeyError(u\"the label [{key}] is not in the [{axis}]\"\n\u001b[1;32m   1784\u001b[0m                                .format(key=key,\n\u001b[0;32m-> 1785\u001b[0;31m                                        axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'the label [seq] is not in the [columns]'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ApAN1mouuLCu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "POsKflAkuLC7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-1Qk_d-zuLDb",
        "colab_type": "code",
        "outputId": "6ef03022-64e4-4124-e714-2b2bfe55ec94",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(models)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "539"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "sRVkIHB5uLDp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"../models/PUC.pickle\", \"wb\") as fp:\n",
        "    pickle.dump(models, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ci27a0EouLDz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}